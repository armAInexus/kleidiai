//
// SPDX-FileCopyrightText: Copyright 2024 Arm Limited and/or its affiliates <open-source-office@arm.com>
//
// SPDX-License-Identifier: Apache-2.0
//
#if !defined(__ARM_FEATURE_MATMUL_INT8)
#error "I8mm extension required to compile this micro-kernel"
#else
#include "kai_matmul_clamp_f32_qai8dxp4x8_qsi4cxp8x8_8x8x32_neon_i8mm.h"

#include <arm_neon.h>
#include <stdint.h>

#include "kai/kai_common.h"

static const size_t kai_m_step = 8;
static const size_t kai_n_step = 8;
static const size_t kai_mr = 4;
static const size_t kai_nr = 8;
static const size_t kai_kr = 16;
static const size_t kai_sr = 2;
static const size_t kai_num_bytes_multiplier_lhs = sizeof(float);
static const size_t kai_num_bytes_multiplier_rhs = sizeof(float);
static const size_t kai_num_bytes_offset_lhs = sizeof(int32_t);
static const size_t kai_num_bytes_sum_rhs = sizeof(int32_t);
static const size_t kai_num_bytes_bias = sizeof(float);

inline static size_t kai_k_roundedup(size_t k) {
    // Since we pack a float and int32 value at the end of the row,
    // we must make sure that k is a multiple of 4 for alignment
    size_t kr_sr_roundedup4 = kai_roundup(kai_kr * kai_sr, 4);
    return kai_roundup(k, kr_sr_roundedup4);
}

inline static size_t kai_lhs_packed_stride(size_t k) {
    const size_t k_internal = kai_k_roundedup(k);

    KAI_ASSERT((k_internal % 2) == 0);

    return kai_mr * (k_internal * sizeof(int8_t) + kai_num_bytes_multiplier_lhs + kai_num_bytes_offset_lhs);
}

inline static size_t kai_rhs_packed_stride(size_t k) {
    const size_t k_internal = kai_k_roundedup(k);

    KAI_ASSERT((k_internal % 2) == 0);

    return kai_nr * ((k_internal / 2) + kai_num_bytes_multiplier_rhs + kai_num_bytes_sum_rhs + kai_num_bytes_bias);
}

size_t kai_get_m_step_matmul_clamp_f32_qai8dxp4x8_qsi4cxp8x8_8x8x32_neon_i8mm(void) {
    return kai_m_step;
}

size_t kai_get_n_step_matmul_clamp_f32_qai8dxp4x8_qsi4cxp8x8_8x8x32_neon_i8mm(void) {
    return kai_n_step;
}

size_t kai_get_mr_matmul_clamp_f32_qai8dxp4x8_qsi4cxp8x8_8x8x32_neon_i8mm(void) {
    return kai_mr;
}

size_t kai_get_nr_matmul_clamp_f32_qai8dxp4x8_qsi4cxp8x8_8x8x32_neon_i8mm(void) {
    return kai_nr;
}

size_t kai_get_kr_matmul_clamp_f32_qai8dxp4x8_qsi4cxp8x8_8x8x32_neon_i8mm(void) {
    return kai_kr;
}

size_t kai_get_sr_matmul_clamp_f32_qai8dxp4x8_qsi4cxp8x8_8x8x32_neon_i8mm(void) {
    return kai_sr;
}

size_t kai_get_lhs_packed_offset_matmul_clamp_f32_qai8dxp4x8_qsi4cxp8x8_8x8x32_neon_i8mm(size_t m_idx, size_t k) {
    KAI_ASSERT((m_idx % kai_m_step) == 0);

    return (m_idx / kai_m_step) * kai_lhs_packed_stride(k);
}

size_t kai_get_rhs_packed_offset_matmul_clamp_f32_qai8dxp4x8_qsi4cxp8x8_8x8x32_neon_i8mm(size_t n_idx, size_t k) {
    KAI_ASSERT((n_idx % kai_n_step) == 0);

    return (n_idx / kai_n_step) * kai_rhs_packed_stride(k);
}

size_t kai_get_dst_offset_matmul_clamp_f32_qai8dxp4x8_qsi4cxp8x8_8x8x32_neon_i8mm(
    size_t m_idx, size_t n_idx, size_t dst_stride) {
    KAI_ASSERT((m_idx % kai_m_step) == 0);
    KAI_ASSERT((n_idx % kai_n_step) == 0);

    return (n_idx * sizeof(float)) + m_idx * dst_stride;
}

size_t kai_get_dst_size_matmul_clamp_f32_qai8dxp4x8_qsi4cxp8x8_8x8x32_neon_i8mm(size_t m, size_t n) {
    return m * n * sizeof(float);
}

void kai_run_matmul_clamp_f32_qai8dxp4x8_qsi4cxp8x8_8x8x32_neon_i8mm(
    size_t m, size_t n, size_t k, const void* lhs_packed, const void* rhs_packed, float* dst, size_t dst_stride_row,
    size_t dst_stride_col, float scalar_min, float scalar_max) {
    KAI_ASSERT(dst_stride_col == sizeof(float));

    if (m == 0) {
        return;
    }

    const size_t k_internal = kai_k_roundedup(k);

    size_t num_blocks = k_internal / 32;

    float clamp_vals[2] = {scalar_min, scalar_max};

    __asm__ __volatile__(
        "mov x12, %x[m]\n"
        "mov x11, #0x80\n"
        "movi v25.16b, #0xf0\n"
        "mov x20, #0x20\n"
        "cmp x12, #0x8\n"
        "madd x11, %x[num_blocks], x11, x20\n"
        "blt 12f\n"
        "1:"  // Row loop
        "mov x10, %x[rhs_packed]\n"
        "mov x9, %x[n]\n"
        "add x28, %x[dst], %x[dst_stride_row], LSL #3\n"
        "2:"  // Column loop
        "mov x22, %x[lhs_packed]\n"
        "movi v29.4s, #0x0\n"
        "movi v12.4s, #0x0\n"
        "mov x21, %x[num_blocks]\n"
        "movi v18.4s, #0x0\n"
        "movi v4.4s, #0x0\n"
        "movi v11.4s, #0x0\n"
        "movi v14.4s, #0x0\n"
        "add x20, x22, x11\n"
        "movi v24.4s, #0x0\n"
        "movi v1.4s, #0x0\n"
        "movi v19.4s, #0x0\n"
        "movi v30.4s, #0x0\n"
        "movi v31.4s, #0x0\n"
        "movi v26.4s, #0x0\n"
        "movi v5.4s, #0x0\n"
        "movi v13.4s, #0x0\n"
        "movi v7.4s, #0x0\n"
        "movi v15.4s, #0x0\n"
        "3:"  // Sub block loop
        "ldr q0, [x10, #0x0]\n"
        "ldr q17, [x10, #0x10]\n"
        "subs x21, x21, #0x1\n"
        "ldr q10, [x10, #0x20]\n"
        "ldr q8, [x10, #0x30]\n"
        "ldr q9, [x22, #0x0]\n"
        "ldr q20, [x22, #0x10]\n"
        "ldr q2, [x20, #0x0]\n"
        "ldr q3, [x20, #0x10]\n"
        "shl v23.16b, v0.16b, #0x4\n"
        "shl v21.16b, v17.16b, #0x4\n"
        "ldr q27, [x10, #0x40]\n"
        "ldr q6, [x10, #0x50]\n"
        "shl v16.16b, v10.16b, #0x4\n"
        "shl v22.16b, v8.16b, #0x4\n"
        "ldr q28, [x10, #0x60]\n"
        "and v0.16b, v0.16b, v25.16b\n"
        "and v17.16b, v17.16b, v25.16b\n"
        ".inst 0x4e97a53d  // smmla v29.4s, v9.16b, v23.16b\n"
        ".inst 0x4e95a532  // smmla v18.4s, v9.16b, v21.16b\n"
        ".inst 0x4e97a68b  // smmla v11.4s, v20.16b, v23.16b\n"
        "and v10.16b, v10.16b, v25.16b\n"
        ".inst 0x4e90a52c  // smmla v12.4s, v9.16b, v16.16b\n"
        ".inst 0x4e96a524  // smmla v4.4s, v9.16b, v22.16b\n"
        "ldr q9, [x10, #0x70]\n"
        "and v8.16b, v8.16b, v25.16b\n"
        ".inst 0x4e95a698  // smmla v24.4s, v20.16b, v21.16b\n"
        ".inst 0x4e90a68e  // smmla v14.4s, v20.16b, v16.16b\n"
        "add x10, x10, #0x80\n"
        ".inst 0x4e96a681  // smmla v1.4s, v20.16b, v22.16b\n"
        "ldr q20, [x22, #0x20]\n"
        ".inst 0x4e97a453  // smmla v19.4s, v2.16b, v23.16b\n"
        ".inst 0x4e95a45f  // smmla v31.4s, v2.16b, v21.16b\n"
        ".inst 0x4e90a45e  // smmla v30.4s, v2.16b, v16.16b\n"
        ".inst 0x4e96a45a  // smmla v26.4s, v2.16b, v22.16b\n"
        "ldr q2, [x22, #0x30]\n"
        ".inst 0x4e97a465  // smmla v5.4s, v3.16b, v23.16b\n"
        "ldr q23, [x20, #0x20]\n"
        ".inst 0x4e95a467  // smmla v7.4s, v3.16b, v21.16b\n"
        "ldr q21, [x20, #0x30]\n"
        ".inst 0x4e90a46d  // smmla v13.4s, v3.16b, v16.16b\n"
        "ldr q16, [x22, #0x40]\n"
        ".inst 0x4e96a46f  // smmla v15.4s, v3.16b, v22.16b\n"
        "ldr q3, [x22, #0x50]\n"
        "shl v22.16b, v27.16b, #0x4\n"
        "and v27.16b, v27.16b, v25.16b\n"
        ".inst 0x4e96a69d  // smmla v29.4s, v20.16b, v22.16b\n"
        ".inst 0x4e96a44b  // smmla v11.4s, v2.16b, v22.16b\n"
        ".inst 0x4e96a6f3  // smmla v19.4s, v23.16b, v22.16b\n"
        ".inst 0x4e96a6a5  // smmla v5.4s, v21.16b, v22.16b\n"
        "shl v22.16b, v6.16b, #0x4\n"
        "and v6.16b, v6.16b, v25.16b\n"
        ".inst 0x4e96a692  // smmla v18.4s, v20.16b, v22.16b\n"
        ".inst 0x4e96a458  // smmla v24.4s, v2.16b, v22.16b\n"
        ".inst 0x4e96a6ff  // smmla v31.4s, v23.16b, v22.16b\n"
        ".inst 0x4e96a6a7  // smmla v7.4s, v21.16b, v22.16b\n"
        "shl v22.16b, v28.16b, #0x4\n"
        ".inst 0x4e80a61d  // smmla v29.4s, v16.16b, v0.16b\n"
        ".inst 0x4e80a46b  // smmla v11.4s, v3.16b, v0.16b\n"
        "and v28.16b, v28.16b, v25.16b\n"
        ".inst 0x4e96a68c  // smmla v12.4s, v20.16b, v22.16b\n"
        ".inst 0x4e96a44e  // smmla v14.4s, v2.16b, v22.16b\n"
        ".inst 0x4e96a6fe  // smmla v30.4s, v23.16b, v22.16b\n"
        ".inst 0x4e96a6ad  // smmla v13.4s, v21.16b, v22.16b\n"
        "shl v22.16b, v9.16b, #0x4\n"
        ".inst 0x4e91a612  // smmla v18.4s, v16.16b, v17.16b\n"
        ".inst 0x4e91a478  // smmla v24.4s, v3.16b, v17.16b\n"
        "and v9.16b, v9.16b, v25.16b\n"
        ".inst 0x4e96a684  // smmla v4.4s, v20.16b, v22.16b\n"
        "ldr q20, [x20, #0x40]\n"
        ".inst 0x4e96a441  // smmla v1.4s, v2.16b, v22.16b\n"
        "ldr q2, [x20, #0x50]\n"
        ".inst 0x4e96a6fa  // smmla v26.4s, v23.16b, v22.16b\n"
        "ldr q23, [x22, #0x60]\n"
        ".inst 0x4e96a6af  // smmla v15.4s, v21.16b, v22.16b\n"
        "ldr q22, [x22, #0x70]\n"
        "ldr q21, [x20, #0x60]\n"
        ".inst 0x4e8aa60c  // smmla v12.4s, v16.16b, v10.16b\n"
        ".inst 0x4e8aa46e  // smmla v14.4s, v3.16b, v10.16b\n"
        "add x22, x22, #0x80\n"
        ".inst 0x4e80a693  // smmla v19.4s, v20.16b, v0.16b\n"
        ".inst 0x4e91a69f  // smmla v31.4s, v20.16b, v17.16b\n"
        ".inst 0x4e88a604  // smmla v4.4s, v16.16b, v8.16b\n"
        "ldr q16, [x20, #0x70]\n"
        ".inst 0x4e88a461  // smmla v1.4s, v3.16b, v8.16b\n"
        "add x20, x20, #0x80\n"
        ".inst 0x4e8aa69e  // smmla v30.4s, v20.16b, v10.16b\n"
        ".inst 0x4e88a69a  // smmla v26.4s, v20.16b, v8.16b\n"
        ".inst 0x4e80a445  // smmla v5.4s, v2.16b, v0.16b\n"
        ".inst 0x4e91a447  // smmla v7.4s, v2.16b, v17.16b\n"
        ".inst 0x4e8aa44d  // smmla v13.4s, v2.16b, v10.16b\n"
        ".inst 0x4e88a44f  // smmla v15.4s, v2.16b, v8.16b\n"
        ".inst 0x4e9ba6fd  // smmla v29.4s, v23.16b, v27.16b\n"
        ".inst 0x4e86a6f2  // smmla v18.4s, v23.16b, v6.16b\n"
        ".inst 0x4e9ca6ec  // smmla v12.4s, v23.16b, v28.16b\n"
        ".inst 0x4e89a6e4  // smmla v4.4s, v23.16b, v9.16b\n"
        ".inst 0x4e9ba6cb  // smmla v11.4s, v22.16b, v27.16b\n"
        ".inst 0x4e86a6d8  // smmla v24.4s, v22.16b, v6.16b\n"
        ".inst 0x4e9ca6ce  // smmla v14.4s, v22.16b, v28.16b\n"
        ".inst 0x4e89a6c1  // smmla v1.4s, v22.16b, v9.16b\n"
        ".inst 0x4e9ba6b3  // smmla v19.4s, v21.16b, v27.16b\n"
        ".inst 0x4e86a6bf  // smmla v31.4s, v21.16b, v6.16b\n"
        ".inst 0x4e9ca6be  // smmla v30.4s, v21.16b, v28.16b\n"
        ".inst 0x4e89a6ba  // smmla v26.4s, v21.16b, v9.16b\n"
        ".inst 0x4e9ba605  // smmla v5.4s, v16.16b, v27.16b\n"
        ".inst 0x4e86a607  // smmla v7.4s, v16.16b, v6.16b\n"
        ".inst 0x4e9ca60d  // smmla v13.4s, v16.16b, v28.16b\n"
        ".inst 0x4e89a60f  // smmla v15.4s, v16.16b, v9.16b\n"
        "bgt 3b\n"
        "ldr q6, [x10, #0x0]\n"
        "ldr q22, [x10, #0x10]\n"
        "uzp1 v9.2d, v29.2d, v18.2d\n"
        "uzp2 v2.2d, v29.2d, v18.2d\n"
        "ld1 { v21.4s }, [x22]\n"
        "ldr q20, [x10, #0x20]\n"
        "uzp1 v16.2d, v12.2d, v4.2d\n"
        "uzp2 v23.2d, v12.2d, v4.2d\n"
        "ldr q10, [x10, #0x30]\n"
        "uzp1 v17.2d, v11.2d, v24.2d\n"
        "uzp2 v24.2d, v11.2d, v24.2d\n"
        "add x22, x22, #0x10\n"
        "ldr q28, [x22, #0x0]\n"
        "uzp1 v0.2d, v14.2d, v1.2d\n"
        "uzp2 v27.2d, v14.2d, v1.2d\n"
        "add x10, x10, #0x40\n"
        "mla v9.4s, v6.4s, v21.s[0]\n"
        "mla v16.4s, v22.4s, v21.s[0]\n"
        "mla v2.4s, v6.4s, v21.s[1]\n"
        "mla v23.4s, v22.4s, v21.s[1]\n"
        "mla v17.4s, v6.4s, v21.s[2]\n"
        "mla v0.4s, v22.4s, v21.s[2]\n"
        "fmul v12.4s, v20.4s, v28.s[0]\n"
        "mla v24.4s, v6.4s, v21.s[3]\n"
        "mla v27.4s, v22.4s, v21.s[3]\n"
        "fmul v11.4s, v10.4s, v28.s[0]\n"
        "scvtf v9.4s, v9.4s\n"
        "scvtf v16.4s, v16.4s\n"
        "fmul v18.4s, v20.4s, v28.s[1]\n"
        "scvtf v2.4s, v2.4s\n"
        "fmul v1.4s, v10.4s, v28.s[1]\n"
        "scvtf v23.4s, v23.4s\n"
        "fmul v14.4s, v20.4s, v28.s[2]\n"
        "scvtf v17.4s, v17.4s\n"
        "fmul v3.4s, v10.4s, v28.s[2]\n"
        "scvtf v0.4s, v0.4s\n"
        "fmul v8.4s, v20.4s, v28.s[3]\n"
        "scvtf v24.4s, v24.4s\n"
        "fmul v28.4s, v10.4s, v28.s[3]\n"
        "scvtf v27.4s, v27.4s\n"
        "fmul v29.4s, v9.4s, v12.4s\n"
        "fmul v12.4s, v16.4s, v11.4s\n"
        "fmul v18.4s, v2.4s, v18.4s\n"
        "fmul v4.4s, v23.4s, v1.4s\n"
        "fmul v11.4s, v17.4s, v14.4s\n"
        "fmul v14.4s, v0.4s, v3.4s\n"
        "fmul v24.4s, v24.4s, v8.4s\n"
        "fmul v1.4s, v27.4s, v28.4s\n"
        "ld1 { v0.4s }, [x20]\n"
        "uzp1 v23.2d, v19.2d, v31.2d\n"
        "uzp2 v2.2d, v19.2d, v31.2d\n"
        "add x20, x20, #0x10\n"
        "ldr q16, [x20, #0x0]\n"
        "uzp1 v3.2d, v30.2d, v26.2d\n"
        "uzp2 v21.2d, v30.2d, v26.2d\n"
        "uzp1 v27.2d, v5.2d, v7.2d\n"
        "uzp2 v9.2d, v5.2d, v7.2d\n"
        "uzp1 v7.2d, v13.2d, v15.2d\n"
        "uzp2 v28.2d, v13.2d, v15.2d\n"
        "mla v23.4s, v6.4s, v0.s[0]\n"
        "mla v3.4s, v22.4s, v0.s[0]\n"
        "mla v2.4s, v6.4s, v0.s[1]\n"
        "fmul v30.4s, v20.4s, v16.s[0]\n"
        "mla v21.4s, v22.4s, v0.s[1]\n"
        "mla v27.4s, v6.4s, v0.s[2]\n"
        "fmul v5.4s, v10.4s, v16.s[0]\n"
        "mla v7.4s, v22.4s, v0.s[2]\n"
        "mla v9.4s, v6.4s, v0.s[3]\n"
        "fmul v15.4s, v20.4s, v16.s[1]\n"
        "mla v28.4s, v22.4s, v0.s[3]\n"
        "scvtf v23.4s, v23.4s\n"
        "scvtf v3.4s, v3.4s\n"
        "scvtf v2.4s, v2.4s\n"
        "fmul v6.4s, v10.4s, v16.s[1]\n"
        "scvtf v21.4s, v21.4s\n"
        "fmul v13.4s, v20.4s, v16.s[2]\n"
        "scvtf v27.4s, v27.4s\n"
        "fmul v8.4s, v10.4s, v16.s[2]\n"
        "scvtf v7.4s, v7.4s\n"
        "fmul v0.4s, v20.4s, v16.s[3]\n"
        "scvtf v9.4s, v9.4s\n"
        "fmul v16.4s, v10.4s, v16.s[3]\n"
        "scvtf v28.4s, v28.4s\n"
        "fmul v19.4s, v23.4s, v30.4s\n"
        "fmul v30.4s, v3.4s, v5.4s\n"
        "fmul v31.4s, v2.4s, v15.4s\n"
        "fmul v26.4s, v21.4s, v6.4s\n"
        "fmul v5.4s, v27.4s, v13.4s\n"
        "fmul v13.4s, v7.4s, v8.4s\n"
        "fmul v7.4s, v9.4s, v0.4s\n"
        "fmul v15.4s, v28.4s, v16.4s\n"
        "ld1r { v3.4s }, [%x[clamp_vals]]\n"
        "add x20, %x[clamp_vals], #0x4\n"
        "cmp x9, #0x8\n"
        "ld1r { v16.4s }, [x20]\n"
        "add x10, x10, #0x20\n"
        "fmax v29.4s, v29.4s, v3.4s\n"
        "fmax v12.4s, v12.4s, v3.4s\n"
        "fmax v18.4s, v18.4s, v3.4s\n"
        "fmax v4.4s, v4.4s, v3.4s\n"
        "fmax v11.4s, v11.4s, v3.4s\n"
        "fmax v14.4s, v14.4s, v3.4s\n"
        "fmax v24.4s, v24.4s, v3.4s\n"
        "fmax v1.4s, v1.4s, v3.4s\n"
        "fmax v19.4s, v19.4s, v3.4s\n"
        "fmax v30.4s, v30.4s, v3.4s\n"
        "fmax v31.4s, v31.4s, v3.4s\n"
        "fmax v26.4s, v26.4s, v3.4s\n"
        "fmax v5.4s, v5.4s, v3.4s\n"
        "fmax v13.4s, v13.4s, v3.4s\n"
        "fmax v7.4s, v7.4s, v3.4s\n"
        "fmax v15.4s, v15.4s, v3.4s\n"
        "fmin v29.4s, v29.4s, v16.4s\n"
        "fmin v12.4s, v12.4s, v16.4s\n"
        "fmin v18.4s, v18.4s, v16.4s\n"
        "fmin v4.4s, v4.4s, v16.4s\n"
        "fmin v11.4s, v11.4s, v16.4s\n"
        "fmin v14.4s, v14.4s, v16.4s\n"
        "fmin v24.4s, v24.4s, v16.4s\n"
        "fmin v1.4s, v1.4s, v16.4s\n"
        "fmin v19.4s, v19.4s, v16.4s\n"
        "fmin v30.4s, v30.4s, v16.4s\n"
        "fmin v31.4s, v31.4s, v16.4s\n"
        "fmin v26.4s, v26.4s, v16.4s\n"
        "fmin v5.4s, v5.4s, v16.4s\n"
        "fmin v13.4s, v13.4s, v16.4s\n"
        "fmin v7.4s, v7.4s, v16.4s\n"
        "fmin v15.4s, v15.4s, v16.4s\n"
        "blt 6f\n"
        "mov x20, %x[dst]\n"
        "str q29, [x20, #0x0]\n"
        "str q12, [x20, #0x10]\n"
        "add x20, x20, %x[dst_stride_row]\n"
        "str q18, [x20, #0x0]\n"
        "str q4, [x20, #0x10]\n"
        "add x20, x20, %x[dst_stride_row]\n"
        "str q11, [x20, #0x0]\n"
        "str q14, [x20, #0x10]\n"
        "add x20, x20, %x[dst_stride_row]\n"
        "str q24, [x20, #0x0]\n"
        "str q1, [x20, #0x10]\n"
        "add x20, x20, %x[dst_stride_row]\n"
        "str q19, [x20, #0x0]\n"
        "str q30, [x20, #0x10]\n"
        "add x20, x20, %x[dst_stride_row]\n"
        "str q31, [x20, #0x0]\n"
        "str q26, [x20, #0x10]\n"
        "add x20, x20, %x[dst_stride_row]\n"
        "str q5, [x20, #0x0]\n"
        "str q13, [x20, #0x10]\n"
        "add x20, x20, %x[dst_stride_row]\n"
        "str q7, [x20, #0x0]\n"
        "str q15, [x20, #0x10]\n"
        "b 11f\n"
        "6:"  // Partial output
        "mov x27, %x[dst]\n"
        "add x26, x27, %x[dst_stride_row], LSL #2\n"
        "add x25, x26, %x[dst_stride_row], LSL #1\n"
        "add x24, x26, %x[dst_stride_row]\n"
        "add x23, x25, %x[dst_stride_row]\n"
        "add x22, x27, %x[dst_stride_row], LSL #1\n"
        "add x21, x27, %x[dst_stride_row]\n"
        "add x20, x22, %x[dst_stride_row]\n"
        "tbz x9, #2, 8f\n"
        "st1 { v7.4s }, [x23], #0x10\n"
        "st1 { v5.4s }, [x25], #0x10\n"
        "st1 { v31.4s }, [x24], #0x10\n"
        "st1 { v19.4s }, [x26], #0x10\n"
        "st1 { v24.4s }, [x20], #0x10\n"
        "st1 { v11.4s }, [x22], #0x10\n"
        "st1 { v18.4s }, [x21], #0x10\n"
        "st1 { v29.4s }, [x27], #0x10\n"
        "tbz x9, #1, 7f\n"
        "st1 { v15.d }[0], [x23], #0x8\n"
        "st1 { v13.d }[0], [x25], #0x8\n"
        "st1 { v26.d }[0], [x24], #0x8\n"
        "st1 { v30.d }[0], [x26], #0x8\n"
        "st1 { v1.d }[0], [x20], #0x8\n"
        "st1 { v14.d }[0], [x22], #0x8\n"
        "st1 { v4.d }[0], [x21], #0x8\n"
        "st1 { v12.d }[0], [x27], #0x8\n"
        "tbz x9, #0, 10f\n"
        "st1 { v15.s }[2], [x23]\n"
        "st1 { v13.s }[2], [x25]\n"
        "st1 { v26.s }[2], [x24]\n"
        "st1 { v30.s }[2], [x26]\n"
        "st1 { v1.s }[2], [x20]\n"
        "st1 { v14.s }[2], [x22]\n"
        "st1 { v4.s }[2], [x21]\n"
        "st1 { v12.s }[2], [x27]\n"
        "b 10f\n"
        "7:"  // Output block 0: partial_1_4
        "tbz x9, #0, 10f\n"
        "st1 { v15.s }[0], [x23]\n"
        "st1 { v13.s }[0], [x25]\n"
        "st1 { v26.s }[0], [x24]\n"
        "st1 { v30.s }[0], [x26]\n"
        "st1 { v1.s }[0], [x20]\n"
        "st1 { v14.s }[0], [x22]\n"
        "st1 { v4.s }[0], [x21]\n"
        "st1 { v12.s }[0], [x27]\n"
        "b 10f\n"
        "8:"  // Output block 0: partial_2_0
        "tbz x9, #1, 9f\n"
        "st1 { v7.d }[0], [x23], #0x8\n"
        "st1 { v5.d }[0], [x25], #0x8\n"
        "st1 { v31.d }[0], [x24], #0x8\n"
        "st1 { v19.d }[0], [x26], #0x8\n"
        "st1 { v24.d }[0], [x20], #0x8\n"
        "st1 { v11.d }[0], [x22], #0x8\n"
        "st1 { v18.d }[0], [x21], #0x8\n"
        "st1 { v29.d }[0], [x27], #0x8\n"
        "tbz x9, #0, 10f\n"
        "st1 { v7.s }[2], [x23]\n"
        "st1 { v5.s }[2], [x25]\n"
        "st1 { v31.s }[2], [x24]\n"
        "st1 { v19.s }[2], [x26]\n"
        "st1 { v24.s }[2], [x20]\n"
        "st1 { v11.s }[2], [x22]\n"
        "st1 { v18.s }[2], [x21]\n"
        "st1 { v29.s }[2], [x27]\n"
        "b 10f\n"
        "9:"  // Output block 0: partial_1_0
        "st1 { v7.s }[0], [x23]\n"
        "st1 { v5.s }[0], [x25]\n"
        "st1 { v31.s }[0], [x24]\n"
        "st1 { v19.s }[0], [x26]\n"
        "st1 { v24.s }[0], [x20]\n"
        "st1 { v11.s }[0], [x22]\n"
        "st1 { v18.s }[0], [x21]\n"
        "st1 { v29.s }[0], [x27]\n"
        "10:"  // Output block 0: Done
        "11:"  // Output stage exit
        "subs x9, x9, #0x8\n"
        "add %x[dst], %x[dst], #0x20\n"
        "bgt 2b\n"
        "mov x20, #0x2\n"
        "sub x12, x12, #0x8\n"
        "cmp x12, #0x8\n"
        "mov %x[dst], x28\n"
        "madd %x[lhs_packed], x20, x11, %x[lhs_packed]\n"
        "bge 1b\n"
        "12:"  // Row loop skip
        "cbz x12, 23f\n"
        "13:"  // Row tail: Row loop
        "mov x26, %x[rhs_packed]\n"
        "mov x25, %x[n]\n"
        "add x24, %x[dst], %x[dst_stride_row], LSL #2\n"
        "14:"  // Row tail: Column loop
        "mov x22, %x[lhs_packed]\n"
        "movi v29.4s, #0x0\n"
        "movi v12.4s, #0x0\n"
        "mov x20, %x[num_blocks]\n"
        "movi v18.4s, #0x0\n"
        "movi v4.4s, #0x0\n"
        "movi v11.4s, #0x0\n"
        "movi v14.4s, #0x0\n"
        "movi v24.4s, #0x0\n"
        "movi v1.4s, #0x0\n"
        "15:"  // Row tail: Sub block loop
        "ldr q16, [x26, #0x0]\n"
        "ldr q7, [x26, #0x10]\n"
        "subs x20, x20, #0x1\n"
        "ldr q6, [x26, #0x20]\n"
        "ldr q17, [x26, #0x30]\n"
        "ldr q20, [x22, #0x0]\n"
        "ldr q3, [x22, #0x10]\n"
        "ldr q26, [x26, #0x40]\n"
        "ldr q21, [x26, #0x50]\n"
        "shl v13.16b, v16.16b, #0x4\n"
        "shl v28.16b, v7.16b, #0x4\n"
        "ldr q31, [x26, #0x60]\n"
        "ldr q8, [x26, #0x70]\n"
        "shl v2.16b, v6.16b, #0x4\n"
        "shl v0.16b, v17.16b, #0x4\n"
        "ldr q10, [x22, #0x20]\n"
        "ldr q9, [x22, #0x30]\n"
        "and v16.16b, v16.16b, v25.16b\n"
        "and v7.16b, v7.16b, v25.16b\n"
        "ldr q27, [x22, #0x40]\n"
        "ldr q23, [x22, #0x50]\n"
        ".inst 0x4e8da69d  // smmla v29.4s, v20.16b, v13.16b\n"
        ".inst 0x4e9ca692  // smmla v18.4s, v20.16b, v28.16b\n"
        "ldr q22, [x22, #0x60]\n"
        "ldr q15, [x22, #0x70]\n"
        ".inst 0x4e82a68c  // smmla v12.4s, v20.16b, v2.16b\n"
        ".inst 0x4e80a684  // smmla v4.4s, v20.16b, v0.16b\n"
        ".inst 0x4e8da46b  // smmla v11.4s, v3.16b, v13.16b\n"
        ".inst 0x4e9ca478  // smmla v24.4s, v3.16b, v28.16b\n"
        "shl v20.16b, v26.16b, #0x4\n"
        "add x26, x26, #0x80\n"
        ".inst 0x4e82a46e  // smmla v14.4s, v3.16b, v2.16b\n"
        ".inst 0x4e80a461  // smmla v1.4s, v3.16b, v0.16b\n"
        "shl v0.16b, v21.16b, #0x4\n"
        "add x22, x22, #0x80\n"
        "shl v5.16b, v31.16b, #0x4\n"
        "shl v13.16b, v8.16b, #0x4\n"
        ".inst 0x4e94a55d  // smmla v29.4s, v10.16b, v20.16b\n"
        "and v6.16b, v6.16b, v25.16b\n"
        "and v17.16b, v17.16b, v25.16b\n"
        ".inst 0x4e80a552  // smmla v18.4s, v10.16b, v0.16b\n"
        ".inst 0x4e94a52b  // smmla v11.4s, v9.16b, v20.16b\n"
        ".inst 0x4e80a538  // smmla v24.4s, v9.16b, v0.16b\n"
        "and v26.16b, v26.16b, v25.16b\n"
        ".inst 0x4e85a54c  // smmla v12.4s, v10.16b, v5.16b\n"
        ".inst 0x4e8da544  // smmla v4.4s, v10.16b, v13.16b\n"
        "and v21.16b, v21.16b, v25.16b\n"
        ".inst 0x4e85a52e  // smmla v14.4s, v9.16b, v5.16b\n"
        ".inst 0x4e8da521  // smmla v1.4s, v9.16b, v13.16b\n"
        "and v31.16b, v31.16b, v25.16b\n"
        ".inst 0x4e90a77d  // smmla v29.4s, v27.16b, v16.16b\n"
        ".inst 0x4e87a772  // smmla v18.4s, v27.16b, v7.16b\n"
        "and v8.16b, v8.16b, v25.16b\n"
        ".inst 0x4e90a6eb  // smmla v11.4s, v23.16b, v16.16b\n"
        ".inst 0x4e87a6f8  // smmla v24.4s, v23.16b, v7.16b\n"
        ".inst 0x4e86a76c  // smmla v12.4s, v27.16b, v6.16b\n"
        ".inst 0x4e91a764  // smmla v4.4s, v27.16b, v17.16b\n"
        ".inst 0x4e86a6ee  // smmla v14.4s, v23.16b, v6.16b\n"
        ".inst 0x4e91a6e1  // smmla v1.4s, v23.16b, v17.16b\n"
        ".inst 0x4e9aa6dd  // smmla v29.4s, v22.16b, v26.16b\n"
        ".inst 0x4e95a6d2  // smmla v18.4s, v22.16b, v21.16b\n"
        ".inst 0x4e9aa5eb  // smmla v11.4s, v15.16b, v26.16b\n"
        ".inst 0x4e95a5f8  // smmla v24.4s, v15.16b, v21.16b\n"
        ".inst 0x4e9fa6cc  // smmla v12.4s, v22.16b, v31.16b\n"
        ".inst 0x4e88a6c4  // smmla v4.4s, v22.16b, v8.16b\n"
        ".inst 0x4e9fa5ee  // smmla v14.4s, v15.16b, v31.16b\n"
        ".inst 0x4e88a5e1  // smmla v1.4s, v15.16b, v8.16b\n"
        "bgt 15b\n"
        "ldr q22, [x26, #0x0]\n"
        "ldr q21, [x26, #0x10]\n"
        "uzp1 v3.2d, v29.2d, v18.2d\n"
        "uzp2 v31.2d, v29.2d, v18.2d\n"
        "ld1 { v7.4s }, [x22]\n"
        "ldr q27, [x26, #0x20]\n"
        "uzp1 v15.2d, v12.2d, v4.2d\n"
        "uzp2 v6.2d, v12.2d, v4.2d\n"
        "ldr q28, [x26, #0x30]\n"
        "uzp1 v8.2d, v11.2d, v24.2d\n"
        "uzp2 v30.2d, v11.2d, v24.2d\n"
        "add x22, x22, #0x10\n"
        "ldr q16, [x22, #0x0]\n"
        "uzp1 v26.2d, v14.2d, v1.2d\n"
        "uzp2 v20.2d, v14.2d, v1.2d\n"
        "add x26, x26, #0x40\n"
        "mla v3.4s, v22.4s, v7.s[0]\n"
        "mla v15.4s, v21.4s, v7.s[0]\n"
        "mla v31.4s, v22.4s, v7.s[1]\n"
        "mla v6.4s, v21.4s, v7.s[1]\n"
        "mla v8.4s, v22.4s, v7.s[2]\n"
        "mla v26.4s, v21.4s, v7.s[2]\n"
        "fmul v23.4s, v27.4s, v16.s[0]\n"
        "mla v30.4s, v22.4s, v7.s[3]\n"
        "mla v20.4s, v21.4s, v7.s[3]\n"
        "fmul v22.4s, v28.4s, v16.s[0]\n"
        "scvtf v3.4s, v3.4s\n"
        "scvtf v15.4s, v15.4s\n"
        "fmul v21.4s, v27.4s, v16.s[1]\n"
        "scvtf v31.4s, v31.4s\n"
        "fmul v10.4s, v28.4s, v16.s[1]\n"
        "scvtf v6.4s, v6.4s\n"
        "fmul v5.4s, v27.4s, v16.s[2]\n"
        "scvtf v8.4s, v8.4s\n"
        "fmul v7.4s, v28.4s, v16.s[2]\n"
        "scvtf v26.4s, v26.4s\n"
        "fmul v0.4s, v27.4s, v16.s[3]\n"
        "scvtf v30.4s, v30.4s\n"
        "fmul v16.4s, v28.4s, v16.s[3]\n"
        "scvtf v20.4s, v20.4s\n"
        "fmul v29.4s, v3.4s, v23.4s\n"
        "fmul v12.4s, v15.4s, v22.4s\n"
        "fmul v18.4s, v31.4s, v21.4s\n"
        "fmul v4.4s, v6.4s, v10.4s\n"
        "fmul v11.4s, v8.4s, v5.4s\n"
        "fmul v14.4s, v26.4s, v7.4s\n"
        "fmul v24.4s, v30.4s, v0.4s\n"
        "fmul v1.4s, v20.4s, v16.4s\n"
        "ld1r { v23.4s }, [%x[clamp_vals]]\n"
        "add x20, %x[clamp_vals], #0x4\n"
        "cmp x25, #0x8\n"
        "ld1r { v16.4s }, [x20]\n"
        "add x26, x26, #0x20\n"
        "fmax v29.4s, v29.4s, v23.4s\n"
        "fmax v12.4s, v12.4s, v23.4s\n"
        "fmax v18.4s, v18.4s, v23.4s\n"
        "fmax v4.4s, v4.4s, v23.4s\n"
        "fmax v11.4s, v11.4s, v23.4s\n"
        "fmax v14.4s, v14.4s, v23.4s\n"
        "fmax v24.4s, v24.4s, v23.4s\n"
        "fmax v1.4s, v1.4s, v23.4s\n"
        "fmin v29.4s, v29.4s, v16.4s\n"
        "fmin v12.4s, v12.4s, v16.4s\n"
        "fmin v18.4s, v18.4s, v16.4s\n"
        "fmin v4.4s, v4.4s, v16.4s\n"
        "fmin v11.4s, v11.4s, v16.4s\n"
        "fmin v14.4s, v14.4s, v16.4s\n"
        "fmin v24.4s, v24.4s, v16.4s\n"
        "fmin v1.4s, v1.4s, v16.4s\n"
        "blt 17f\n"
        "mov x20, %x[dst]\n"
        "cmp x12, #0x1\n"
        "str q29, [x20, #0x0]\n"
        "str q12, [x20, #0x10]\n"
        "add x20, x20, %x[dst_stride_row]\n"
        "ble 22f\n"
        "cmp x12, #0x2\n"
        "str q18, [x20, #0x0]\n"
        "str q4, [x20, #0x10]\n"
        "add x20, x20, %x[dst_stride_row]\n"
        "ble 22f\n"
        "cmp x12, #0x3\n"
        "str q11, [x20, #0x0]\n"
        "str q14, [x20, #0x10]\n"
        "add x20, x20, %x[dst_stride_row]\n"
        "ble 22f\n"
        "str q24, [x20, #0x0]\n"
        "str q1, [x20, #0x10]\n"
        "b 22f\n"
        "17:"  // Row tail: Partial output
        "mov x23, %x[dst]\n"
        "cmp x12, #0x1\n"
        "add x22, x23, %x[dst_stride_row]\n"
        "csel x22, x22, x23, GE\n"
        "cmp x12, #0x2\n"
        "add x21, x23, %x[dst_stride_row], LSL #1\n"
        "csel x21, x21, x22, GE\n"
        "cmp x12, #0x3\n"
        "add x20, x21, %x[dst_stride_row]\n"
        "csel x20, x20, x21, GE\n"
        "tbz x25, #2, 19f\n"
        "st1 { v24.4s }, [x20], #0x10\n"
        "st1 { v11.4s }, [x21], #0x10\n"
        "st1 { v18.4s }, [x22], #0x10\n"
        "st1 { v29.4s }, [x23], #0x10\n"
        "tbz x25, #1, 18f\n"
        "st1 { v1.d }[0], [x20], #0x8\n"
        "st1 { v14.d }[0], [x21], #0x8\n"
        "st1 { v4.d }[0], [x22], #0x8\n"
        "st1 { v12.d }[0], [x23], #0x8\n"
        "tbz x25, #0, 21f\n"
        "st1 { v1.s }[2], [x20]\n"
        "st1 { v14.s }[2], [x21]\n"
        "st1 { v4.s }[2], [x22]\n"
        "st1 { v12.s }[2], [x23]\n"
        "b 21f\n"
        "18:"  // Row tail: Output block 0: partial_1_4
        "tbz x25, #0, 21f\n"
        "st1 { v1.s }[0], [x20]\n"
        "st1 { v14.s }[0], [x21]\n"
        "st1 { v4.s }[0], [x22]\n"
        "st1 { v12.s }[0], [x23]\n"
        "b 21f\n"
        "19:"  // Row tail: Output block 0: partial_2_0
        "tbz x25, #1, 20f\n"
        "st1 { v24.d }[0], [x20], #0x8\n"
        "st1 { v11.d }[0], [x21], #0x8\n"
        "st1 { v18.d }[0], [x22], #0x8\n"
        "st1 { v29.d }[0], [x23], #0x8\n"
        "tbz x25, #0, 21f\n"
        "st1 { v24.s }[2], [x20]\n"
        "st1 { v11.s }[2], [x21]\n"
        "st1 { v18.s }[2], [x22]\n"
        "st1 { v29.s }[2], [x23]\n"
        "b 21f\n"
        "20:"  // Row tail: Output block 0: partial_1_0
        "st1 { v24.s }[0], [x20]\n"
        "st1 { v11.s }[0], [x21]\n"
        "st1 { v18.s }[0], [x22]\n"
        "st1 { v29.s }[0], [x23]\n"
        "21:"  // Row tail: Output block 0: Done
        "22:"  // Row tail: Output stage exit
        "subs x25, x25, #0x8\n"
        "add %x[dst], %x[dst], #0x20\n"
        "bgt 14b\n"
        "subs x12, x12, #0x4\n"
        "add %x[lhs_packed], %x[lhs_packed], x11\n"
        "mov %x[dst], x24\n"
        "bgt 13b\n"
        "23:"  // Row tail: Row loop skip
        : [dst] "+&r"(dst), [lhs_packed] "+&r"(lhs_packed)
        : [clamp_vals] "r"(clamp_vals), [dst_stride_row] "r"(dst_stride_row), [m] "r"(m), [n] "r"(n),
          [num_blocks] "r"(num_blocks), [rhs_packed] "r"(rhs_packed)
        : "cc", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8", "v9", "v10", "v11", "v12", "v13", "v14",
          "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29",
          "v30", "v31", "x9", "x10", "x11", "x12", "x20", "x21", "x22", "x23", "x24", "x25", "x26", "x27", "x28");
}
#endif  // Architectural feature check
